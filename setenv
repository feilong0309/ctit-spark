export PROJECT_HOME=$PWD
export LIBJARS=$(echo $PROJECT_HOME/target/lib/*.jar | tr ' ' ',')
export CLASS_PREFIX=nl.utwente.bigdata
export HADOOP_CONF_DIR=/etc/hadoop/conf
export EXECUTORS=45
export EXECUTOR_MEM=4g
export DRIVER_MEM=6g
export CORES=4
export USER=$(whoami)

# run tool with yarn-resource manager choosing a worker as the driver
function runTool() {
        name=$1
        shift
        /usr/lib/spark/bin/spark-submit --class $CLASS_PREFIX.$name \
            --name $name
            --master yarn-cluster \
            --num-executors $EXECUTORS \
            --driver-memory $DRIVER_MEM \
            --executor-memory $EXECUTOR_MEM \
            --executor-cores $CORES \
            --queue $USER \
            target/ctit-spark*.jar \
            $@
}
# run tool with the local machine being the driver program.
function runToolClient() {
        name=$1
        shift
        /usr/lib/spark/bin/spark-submit --class $CLASS_PREFIX.$name \
            --master yarn-client \
            --num-executors $EXECUTORS \
            --driver-memory $DRIVER_MEM \
            --executor-memory $EXECUTOR_MEM \
            --executor-cores $CORES \
            --queue $USER \
            target/ctit-spark*.jar \
            $@
}
# create a shell
function runShell() {
        /usr/lib/spark/bin/spark-shell \
            --master yarn-client \
            --num-executors $EXECUTORS \
            --driver-memory $DRIVER_MEM \
            --executor-memory $EXECUTOR_MEM \
            --executor-cores $CORES \
            --conf spark.ui.port=4050 \
            --queue $USER \
            $@ 
}
